"""
RAG Agent æœåŠ¡å±‚
================
èŒè´£ï¼š
- æ„å»ºå¹¶ç¼“å­˜ LangChain Retrieverï¼ˆé™„ç€åˆ°ç°æœ‰ Chroma é›†åˆï¼Œä»…è¯»å–ï¼‰ã€‚
- æ³¨å†Œæ£€ç´¢å·¥å…·å¹¶åˆ›å»º Agentï¼ˆcreate_agentï¼‰ã€‚
- é›†æˆçŸ­æœŸè®°å¿†ï¼ˆcheckpointerï¼‰å’Œè‡ªåŠ¨ Summarizationã€‚
- å¯¹å¤–æš´éœ²æ ‡å‡†è°ƒç”¨æ¥å£ï¼šinvokeï¼ˆä¸€æ¬¡æ€§ï¼‰ã€stream_updatesï¼ˆæ­¥éª¤æµï¼‰ã€stream_messagesï¼ˆä»…æ¨¡å‹æ–‡æœ¬ï¼‰ã€‚

ç”¨æ³•ï¼š
- åœ¨ API å±‚è°ƒç”¨ `stream_messages(question, thread_id="1")` ç›´æ¥å‘å‰ç«¯æ¨æµæ¨¡å‹æ–‡æœ¬ï¼›
- æˆ–è°ƒç”¨ `stream_updates(question, thread_id="1")` æŸ¥çœ‹ model â†’ tools â†’ model çš„æ­¥éª¤è¿›å±•ï¼›
- æˆ–è°ƒç”¨ `invoke(question, thread_id="1")` è·å–å®Œæ•´å›ç­”å­—ç¬¦ä¸²ã€‚

è®°å¿†ç®¡ç†ï¼š
- ä½¿ç”¨ SQLite æ•°æ®åº“æŒä¹…åŒ–å¯¹è¯å†å²ï¼ˆå­˜å‚¨åœ¨ data/chat_memory.dbï¼‰ã€‚
- å½“ token æ•°è¶…è¿‡é˜ˆå€¼æ—¶ï¼Œè‡ªåŠ¨è§¦å‘ Summarization å‹ç¼©å†å²æ¶ˆæ¯ã€‚
- thread_id ç”¨äºåŒºåˆ†ä¸åŒä¼šè¯ï¼Œé»˜è®¤ä½¿ç”¨ "1"ã€‚
"""

import logging
import os
from typing import Iterator, List, Optional

from langchain_core.embeddings import Embeddings
from langchain.tools import tool
from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware
from .. import config
from ..services.embedding_service import EmbeddingService
from ..services.vector_store_repository import VectorStoreRepository
from .llm_handler import LLMHandler


logger = logging.getLogger(__name__)


# ---------------------------- Embeddings é€‚é…å™¨ ----------------------------
class LCEmbeddingAdapter(Embeddings):
    """å°†é¡¹ç›®å†…çš„ EmbeddingService é€‚é…ä¸º LangChain Embeddings æ¥å£ã€‚"""

    def __init__(self, service: EmbeddingService):
        self._service = service

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return self._service.embed_texts(texts)

    def embed_query(self, text: str) -> List[float]:
        return self._service.embed_text(text)


# ---------------------------- æ¨¡å—çº§ç¼“å­˜ ----------------------------
_retriever = None
_agent = None
_checkpointer = None
# ä¸ºä¸åŒ session_id ç¼“å­˜ retriever å’Œ agent
_retriever_cache = {}  # {session_id: retriever}
_agent_cache = {}      # {session_id: agent}


def _get_retriever():
    """æ„å»ºæˆ–è¿”å›ç¼“å­˜çš„ LangChain Retrieverï¼ˆæ— è¿‡æ»¤ï¼Œå‘åå…¼å®¹ï¼‰ã€‚"""
    global _retriever
    if _retriever is not None:
        return _retriever

    embedding_service = EmbeddingService.get_instance()
    vector_repo = VectorStoreRepository()
    lc_embeddings = LCEmbeddingAdapter(embedding_service)

    # ä»…é™„ç€åˆ°æœ¬åœ° Chroma é›†åˆï¼Œè´Ÿè´£"è¯»"
    _retriever = vector_repo.as_langchain_retriever(
        embedding_instance=lc_embeddings,
        search_type="similarity",
        search_kwargs={"k": 5},
    )
    logger.info("RAG retriever å·²åˆ›å»ºå¹¶ç¼“å­˜ã€‚")
    return _retriever


def _get_retriever_with_filter(session_id: str = "1"):
    """
    æ„å»ºæˆ–è¿”å›ç¼“å­˜çš„å¸¦ session_id è¿‡æ»¤çš„ Retrieverã€‚
    
    Args:
        session_id: å½“å‰ä¼šè¯IDï¼Œé»˜è®¤ "1"
        
    æ£€ç´¢èŒƒå›´ï¼š
        - session_id = "system" çš„æ–‡æ¡£ï¼ˆå…¨å±€ç³»ç»Ÿæ–‡æ¡£ï¼‰
        - session_id = å½“å‰ä¼šè¯ID çš„æ–‡æ¡£ï¼ˆç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£ï¼‰
    """
    global _retriever_cache
    
    # æ£€æŸ¥ç¼“å­˜
    if session_id in _retriever_cache:
        logger.info(f"âœ… ä½¿ç”¨ç¼“å­˜çš„ retrieverï¼Œsession_id={session_id}")
        return _retriever_cache[session_id]
    
    embedding_service = EmbeddingService.get_instance()
    vector_repo = VectorStoreRepository()
    lc_embeddings = LCEmbeddingAdapter(embedding_service)
    
    # æ„å»ºè¿‡æ»¤æ¡ä»¶ï¼šæ£€ç´¢ system æ–‡æ¡£ + å½“å‰ä¼šè¯æ–‡æ¡£
    search_kwargs = {
        "k": 5,
        "filter": {
            "$or": [
                {"session_id": "system"},      # ç³»ç»Ÿå…¨å±€æ–‡æ¡£
                {"session_id": session_id}     # å½“å‰ä¼šè¯æ–‡æ¡£
            ]
        }
    }
    
    logger.info(f"ğŸ”¨ åˆ›å»ºæ–°çš„å¸¦è¿‡æ»¤çš„ retrieverï¼Œsession_id={session_id}")
    retriever = vector_repo.as_langchain_retriever(
        embedding_instance=lc_embeddings,
        search_type="similarity",
        search_kwargs=search_kwargs,
    )
    
    # ç¼“å­˜
    _retriever_cache[session_id] = retriever
    return retriever


@tool("retrieve_context", response_format="content_and_artifact")
def retrieve_context(query: str):
    """æ£€ç´¢ä¸é—®é¢˜ç›¸å…³çš„ä¸Šä¸‹æ–‡å†…å®¹ã€‚è¿”å›æ‹¼æ¥æ–‡æœ¬ä»¥åŠåŸå§‹æ–‡æ¡£åˆ—è¡¨ã€‚"""
    retriever = _get_retriever()
    docs = retriever.invoke(query)
    serialized = "\n\n".join(
        (
            f"Source: {d.metadata.get('source', '<unknown>')}\nContent: {d.page_content}"
            for d in docs
        )
    )
    return serialized, docs


def _get_checkpointer():
    """æ„å»ºæˆ–è¿”å›ç¼“å­˜çš„ Checkpointerï¼ˆç”¨äºçŸ­æœŸè®°å¿†æŒä¹…åŒ–ï¼‰ã€‚"""
    global _checkpointer
    if _checkpointer is not None:
        return _checkpointer
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    db_dir = os.path.dirname(config.CHAT_MEMORY_DB_PATH)
    if db_dir:
        os.makedirs(db_dir, exist_ok=True)
    
    # å°è¯•ä½¿ç”¨ SQLite Checkpointerï¼ˆéœ€è¦ langgraph-checkpoint-sqliteï¼‰
    # å¦‚æœä¸å¯ç”¨ï¼Œé™çº§åˆ° MemorySaverï¼ˆå†…å­˜å­˜å‚¨ï¼‰
    try:
        # æ³¨æ„ï¼šSQLite checkpointer å¯èƒ½éœ€è¦é¢å¤–å®‰è£… langgraph-checkpoint-sqlite
        # æˆ–è€…ä½¿ç”¨ langgraph.checkpoint.sqlite.SqliteSaver
        # å…ˆå°è¯•ä»æ ‡å‡†åŒ…å¯¼å…¥
        try:
            from langgraph.checkpoint.sqlite import SqliteSaver
            import sqlite3
            # ç›´æ¥ä½¿ç”¨ sqlite3.connectï¼Œä¸èµ° from_conn_string çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨
            conn = sqlite3.connect(config.CHAT_MEMORY_DB_PATH, check_same_thread=False)
            _checkpointer = SqliteSaver(conn)
            # åˆ›å»ºè¡¨ç»“æ„ï¼ˆè‹¥ä¸å­˜åœ¨ï¼‰
            _checkpointer.setup()
            logger.info(f"ä½¿ç”¨ SQLite Checkpointerï¼Œæ•°æ®åº“è·¯å¾„: {config.CHAT_MEMORY_DB_PATH}")
        except Exception as e:
            # æ•è·å¹¶æ‰“å°è¯¦ç»†å¼‚å¸¸ï¼Œå†é™çº§åˆ° MemorySaver
            logger.error(
                "åˆå§‹åŒ– SqliteSaver å¤±è´¥ï¼Œå°†é™çº§ä¸º MemorySaverã€‚é”™è¯¯: %s: %s",
                type(e).__name__, str(e), exc_info=True
            )
            from langgraph.checkpoint.memory import MemorySaver
            _checkpointer = MemorySaver()
            logger.warning(
                "å·²åˆ‡æ¢ä¸º MemorySaverï¼ˆå†…å­˜å­˜å‚¨ï¼Œé‡å¯åä¸¢å¤±ï¼‰ã€‚"
            )
    except Exception as e:
        # å…œåº•ï¼šä½¿ç”¨ MemorySaver
        from langgraph.checkpoint.memory import MemorySaver
        _checkpointer = MemorySaver()
        logger.warning(f"åˆå§‹åŒ– Checkpointer å¤±è´¥: {e}ï¼Œä½¿ç”¨ MemorySaverï¼ˆå†…å­˜å­˜å‚¨ï¼‰")
    
    return _checkpointer


def _get_agent(session_id: str = "1"):
    """
    æ„å»ºæˆ–è¿”å›ç¼“å­˜çš„ Agentï¼ˆé›†æˆçŸ­æœŸè®°å¿†å’Œ Summarizationï¼‰ã€‚
    
    Args:
        session_id: ä¼šè¯IDï¼Œç”¨äºæ–‡æ¡£æ£€ç´¢è¿‡æ»¤
    
    æ³¨æ„ï¼šä½¿ç”¨ç¼“å­˜æœºåˆ¶ï¼Œç›¸åŒ session_id å¤ç”¨åŒä¸€ä¸ª agent å®ä¾‹
    """
    global _agent_cache
    
    # æ£€æŸ¥ç¼“å­˜
    if session_id in _agent_cache:
        logger.info(f"âœ… ä½¿ç”¨ç¼“å­˜çš„ agentï¼Œsession_id={session_id}")
        return _agent_cache[session_id]
    
    llm = LLMHandler.get_instance().get_model()
    checkpointer = _get_checkpointer()
    
    # åˆ›å»º Summarization Middlewareï¼ˆè‡ªåŠ¨å‹ç¼©å†å²ï¼‰
    summarization_middleware = SummarizationMiddleware(
        model=llm,
        max_tokens_before_summary=config.MEMORY_MAX_TOKENS_BEFORE_SUMMARY,
        messages_to_keep=config.MEMORY_MESSAGES_TO_KEEP,  
    )

    logger.info(f"ğŸ”¨ åˆ›å»ºæ–°çš„ Agentï¼Œsession_id={session_id}, max_tokens={config.MEMORY_MAX_TOKENS_BEFORE_SUMMARY}, keep={config.MEMORY_MESSAGES_TO_KEEP}")
    
    # ã€å…³é”®ã€‘æ ¹æ® session_id åˆ›å»ºå¸¦è¿‡æ»¤çš„ retrieverï¼ˆä¼šå¤ç”¨ç¼“å­˜ï¼‰
    retriever = _get_retriever_with_filter(session_id=session_id)
    
    # ã€å…³é”®ã€‘ä½¿ç”¨é—­åŒ…åˆ›å»ºåŠ¨æ€å·¥å…·ï¼ˆç»‘å®šå½“å‰ session_id çš„ retrieverï¼‰
    @tool("retrieve_context", response_format="content_and_artifact")
    def retrieve_context_filtered(query: str):
        """æ£€ç´¢ä¸é—®é¢˜ç›¸å…³çš„ä¸Šä¸‹æ–‡å†…å®¹ï¼ˆé™å®šå½“å‰ä¼šè¯èŒƒå›´ï¼šç³»ç»Ÿæ–‡æ¡£+ç”¨æˆ·ä¸Šä¼ æ–‡æ¡£ï¼‰ã€‚"""
        docs = retriever.invoke(query)
        serialized = "\n\n".join(
            (
                f"Source: {d.metadata.get('source', '<unknown>')}\n"
                f"Session: {d.metadata.get('session_id', 'unknown')}\n"
                f"Content: {d.page_content}"
                for d in docs
            )
        )
        return serialized, docs
    
    system_prompt = (
        "ä½ æœ‰ä¸€ä¸ªç”¨äºæ£€ç´¢ä¸Šä¸‹æ–‡çš„å·¥å…· retrieve_contextã€‚"
        "è¯·åœ¨éœ€è¦å¤–éƒ¨çŸ¥è¯†æ—¶è°ƒç”¨è¯¥å·¥å…·ï¼Œå¹¶åŸºäºè¿”å›çš„å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
        "æ— æ³•ä»ä¸Šä¸‹æ–‡ç¡®å®šç­”æ¡ˆæ—¶è¯·æ˜ç¡®è¯´æ˜ã€‚\n\n"
        "å›ç­”æ ¼å¼è¦æ±‚ï¼š\n"
        "1. ä½¿ç”¨è§„èŒƒçš„ Markdown æ ¼å¼ï¼Œç¡®ä¿æ®µè½ä¹‹é—´æœ‰ç©ºè¡Œåˆ†éš”ã€‚\n"
        "2. åˆ—è¡¨é¡¹å‰åè¦æœ‰ç©ºè¡Œï¼Œä½¿ç”¨ `-` æˆ– `1.` å¼€å¤´ã€‚\n"
        "3. ä»£ç å—ä½¿ç”¨ä¸‰ä¸ªåå¼•å·åŒ…è£¹ï¼Œå¹¶æ ‡æ³¨è¯­è¨€ã€‚\n"
        "4. é•¿æ®µè½è¦é€‚å½“åˆ†æ®µï¼Œæé«˜å¯è¯»æ€§ã€‚"
    )
    
    agent = create_agent(
        llm,
        [retrieve_context_filtered],  # ä½¿ç”¨å¸¦è¿‡æ»¤çš„å·¥å…·
        system_prompt=system_prompt,
        checkpointer=checkpointer,
        middleware=[summarization_middleware],
    )
    logger.info(f"âœ… RAG agent å·²åˆ›å»ºå¹¶ç¼“å­˜ï¼ˆsession_id={session_id}ï¼Œå«çŸ­æœŸè®°å¿†å’Œ Summarizationï¼‰")
    
    # ç¼“å­˜
    _agent_cache[session_id] = agent
    return agent


# ---------------------------- ç¼“å­˜ç®¡ç† ----------------------------
def clear_cache(session_id: Optional[str] = None):
    """
    æ¸…ç†ç¼“å­˜ã€‚
    
    Args:
        session_id: å¦‚æœæŒ‡å®šï¼Œåªæ¸…ç†è¯¥ session çš„ç¼“å­˜ï¼›å¦‚æœä¸º Noneï¼Œæ¸…ç†æ‰€æœ‰ç¼“å­˜
    """
    global _retriever_cache, _agent_cache
    
    if session_id is None:
        # æ¸…ç†æ‰€æœ‰ç¼“å­˜
        _retriever_cache.clear()
        _agent_cache.clear()
        logger.info("ğŸ—‘ï¸ å·²æ¸…ç†æ‰€æœ‰ session çš„ agent å’Œ retriever ç¼“å­˜")
    else:
        # æ¸…ç†æŒ‡å®š session çš„ç¼“å­˜
        if session_id in _retriever_cache:
            del _retriever_cache[session_id]
        if session_id in _agent_cache:
            del _agent_cache[session_id]
        logger.info(f"ğŸ—‘ï¸ å·²æ¸…ç† session_id={session_id} çš„ agent å’Œ retriever ç¼“å­˜")


# ---------------------------- å¯¹å¤–æ¥å£ ----------------------------
def invoke(question: str, thread_id: str = "1", timeout_s: Optional[float] = None) -> str:
    """
    ä¸€æ¬¡æ€§è°ƒç”¨ï¼Œè¿”å›å®Œæ•´å›ç­”æ–‡æœ¬ã€‚
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        thread_id: å¯¹è¯çº¿ç¨‹IDï¼Œç”¨äºåŒºåˆ†ä¸åŒä¼šè¯å’Œæ–‡æ¡£æ£€ç´¢èŒƒå›´ï¼ˆé»˜è®¤ "1"ï¼‰
        timeout_s: è¶…æ—¶æ—¶é—´ï¼ˆæš‚æœªä½¿ç”¨ï¼‰
    
    Returns:
        å®Œæ•´å›ç­”æ–‡æœ¬
    """
    # ã€ä¿®æ”¹ã€‘ä¼ å…¥ thread_id ä½œä¸º session_idï¼Œç”¨äºæ–‡æ¡£è¿‡æ»¤
    agent = _get_agent(session_id=thread_id)
    config_dict = {"configurable": {"thread_id": thread_id}}
    
    # é‡‡ç”¨ messages æµï¼Œåªæ‹¼æ¥æ¨¡å‹æ–‡æœ¬å—
    parts: List[str] = []
    for token, meta in agent.stream(
        {"messages": [{"role": "user", "content": question}]},
        stream_mode="messages",
        config=config_dict,
    ):
        if isinstance(meta, dict) and meta.get("langgraph_node") != "model":
            continue
        for block in getattr(token, "content_blocks", []) or []:
            if isinstance(block, dict) and block.get("type") == "text":
                text = block.get("text", "")
                if text:
                    parts.append(text)
    return "".join(parts)


def stream_updates(question: str, thread_id: str = "1"):
    """
    æ­¥éª¤çº§æµï¼ˆmodel â†’ tools â†’ modelï¼‰ã€‚äº§å‡º dictï¼Œä¾¿äºè°ƒè¯•ä¸è§‚æµ‹ã€‚
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        thread_id: å¯¹è¯çº¿ç¨‹IDï¼Œç”¨äºåŒºåˆ†ä¸åŒä¼šè¯å’Œæ–‡æ¡£æ£€ç´¢èŒƒå›´ï¼ˆé»˜è®¤ "1"ï¼‰
    
    Yields:
        æ­¥éª¤æ›´æ–°å­—å…¸
    """
    # ã€ä¿®æ”¹ã€‘ä¼ å…¥ thread_id ä½œä¸º session_idï¼Œç”¨äºæ–‡æ¡£è¿‡æ»¤
    agent = _get_agent(session_id=thread_id)
    config_dict = {"configurable": {"thread_id": thread_id}}
    for chunk in agent.stream(
        {"messages": [{"role": "user", "content": question}]},
        stream_mode="updates",
        config=config_dict,
    ):
        yield chunk


def stream_messages(question: str, thread_id: str = "1"):
    """
    ä»…æµå¼è¾“å‡ºæ¨¡å‹æ–‡æœ¬å—ï¼ˆé€æ®µï¼‰ã€‚äº§å‡º strã€‚
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        thread_id: å¯¹è¯çº¿ç¨‹IDï¼Œç”¨äºåŒºåˆ†ä¸åŒä¼šè¯å’Œæ–‡æ¡£æ£€ç´¢èŒƒå›´ï¼ˆé»˜è®¤ "1"ï¼‰
    
    Yields:
        æ–‡æœ¬å—å­—ç¬¦ä¸²
    """
    # ã€ä¿®æ”¹ã€‘ä¼ å…¥ thread_id ä½œä¸º session_idï¼Œç”¨äºæ–‡æ¡£è¿‡æ»¤
    agent = _get_agent(session_id=thread_id)
    config_dict = {"configurable": {"thread_id": thread_id}}
    for token, meta in agent.stream(
        {"messages": [{"role": "user", "content": question}]},
        stream_mode="messages",
        config=config_dict,
    ):
        if isinstance(meta, dict) and meta.get("langgraph_node") != "model":
            continue
        blocks = getattr(token, "content_blocks", None)
        if not blocks:
            continue
        for block in blocks:
            if isinstance(block, dict) and block.get("type") == "text":
                text = block.get("text", "")
                if text:
                    yield text


