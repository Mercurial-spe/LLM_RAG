"""
RAG Agent æœåŠ¡å±‚
================
èŒè´£ï¼š
- æ„å»ºå¹¶ç¼“å­˜ LangChain Retrieverï¼ˆé™„ç€åˆ°ç°æœ‰ Chroma é›†åˆï¼Œä»…è¯»å–ï¼‰ã€‚
- æ³¨å†Œæ£€ç´¢å·¥å…·å¹¶åˆ›å»º Agentï¼ˆcreate_agentï¼‰ã€‚
- é›†æˆçŸ­æœŸè®°å¿†ï¼ˆcheckpointerï¼‰å’Œè‡ªåŠ¨ Summarizationã€‚
- å¯¹å¤–æš´éœ²æ ‡å‡†è°ƒç”¨æ¥å£ï¼šinvokeï¼ˆä¸€æ¬¡æ€§ï¼‰ã€stream_updatesï¼ˆæ­¥éª¤æµï¼‰ã€stream_messagesï¼ˆä»…æ¨¡å‹æ–‡æœ¬ï¼‰ã€‚

ç”¨æ³•ï¼š
- åœ¨ API å±‚è°ƒç”¨ `stream_messages(question, thread_id="1")` ç›´æ¥å‘å‰ç«¯æ¨æµæ¨¡å‹æ–‡æœ¬ï¼›
- æˆ–è°ƒç”¨ `stream_updates(question, thread_id="1")` æŸ¥çœ‹ model â†’ tools â†’ model çš„æ­¥éª¤è¿›å±•ï¼›
- æˆ–è°ƒç”¨ `invoke(question, thread_id="1")` è·å–å®Œæ•´å›ç­”å­—ç¬¦ä¸²ã€‚

è®°å¿†ç®¡ç†ï¼š
- ä½¿ç”¨ SQLite æ•°æ®åº“æŒä¹…åŒ–å¯¹è¯å†å²ï¼ˆå­˜å‚¨åœ¨ data/chat_memory.dbï¼‰ã€‚
- å½“ token æ•°è¶…è¿‡é˜ˆå€¼æ—¶ï¼Œè‡ªåŠ¨è§¦å‘ Summarization å‹ç¼©å†å²æ¶ˆæ¯ã€‚
- thread_id ç”¨äºåŒºåˆ†ä¸åŒä¼šè¯ï¼Œé»˜è®¤ä½¿ç”¨ "1"ã€‚
"""

import logging
import os
from typing import Iterator, List, Optional

from langchain_core.embeddings import Embeddings
from langchain.tools import tool
from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware
from .. import config
from ..services.embedding_service import EmbeddingService
from ..services.vector_store_repository import VectorStoreRepository
from .llm_handler import LLMHandler


logger = logging.getLogger(__name__)


# ---------------------------- Embeddings é€‚é…å™¨ ----------------------------
class LCEmbeddingAdapter(Embeddings):
    """å°†é¡¹ç›®å†…çš„ EmbeddingService é€‚é…ä¸º LangChain Embeddings æ¥å£ã€‚"""

    def __init__(self, service: EmbeddingService):
        self._service = service

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return self._service.embed_texts(texts)

    def embed_query(self, text: str) -> List[float]:
        return self._service.embed_text(text)


# ---------------------------- æ¨¡å—çº§ç¼“å­˜ ----------------------------
_retriever = None
_agent = None
_checkpointer = None


def _get_retriever():
    """æ„å»ºæˆ–è¿”å›ç¼“å­˜çš„ LangChain Retrieverã€‚"""
    global _retriever
    if _retriever is not None:
        return _retriever

    embedding_service = EmbeddingService.get_instance()
    vector_repo = VectorStoreRepository()
    lc_embeddings = LCEmbeddingAdapter(embedding_service)

    # ä»…é™„ç€åˆ°æœ¬åœ° Chroma é›†åˆï¼Œè´Ÿè´£â€œè¯»â€
    _retriever = vector_repo.as_langchain_retriever(
        embedding_instance=lc_embeddings,
        search_type="similarity",
        search_kwargs={"k": 5},
    )
    logger.info("RAG retriever å·²åˆ›å»ºå¹¶ç¼“å­˜ã€‚")
    return _retriever


@tool("retrieve_context", response_format="content_and_artifact")
def retrieve_context(query: str):
    """æ£€ç´¢ä¸é—®é¢˜ç›¸å…³çš„ä¸Šä¸‹æ–‡å†…å®¹ã€‚è¿”å›æ‹¼æ¥æ–‡æœ¬ä»¥åŠåŸå§‹æ–‡æ¡£åˆ—è¡¨ã€‚"""
    retriever = _get_retriever()
    docs = retriever.invoke(query)
    serialized = "\n\n".join(
        (
            f"Source: {d.metadata.get('source', '<unknown>')}\nContent: {d.page_content}"
            for d in docs
        )
    )
    return serialized, docs


def _get_checkpointer():
    """æ„å»ºæˆ–è¿”å›ç¼“å­˜çš„ Checkpointerï¼ˆç”¨äºçŸ­æœŸè®°å¿†æŒä¹…åŒ–ï¼‰ã€‚"""
    global _checkpointer
    if _checkpointer is not None:
        return _checkpointer
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    db_dir = os.path.dirname(config.CHAT_MEMORY_DB_PATH)
    if db_dir:
        os.makedirs(db_dir, exist_ok=True)
    
    # å°è¯•ä½¿ç”¨ SQLite Checkpointerï¼ˆéœ€è¦ langgraph-checkpoint-sqliteï¼‰
    # å¦‚æœä¸å¯ç”¨ï¼Œé™çº§åˆ° MemorySaverï¼ˆå†…å­˜å­˜å‚¨ï¼‰
    try:
        # æ³¨æ„ï¼šSQLite checkpointer å¯èƒ½éœ€è¦é¢å¤–å®‰è£… langgraph-checkpoint-sqlite
        # æˆ–è€…ä½¿ç”¨ langgraph.checkpoint.sqlite.SqliteSaver
        # å…ˆå°è¯•ä»æ ‡å‡†åŒ…å¯¼å…¥
        try:
            from langgraph.checkpoint.sqlite import SqliteSaver
            import sqlite3
            # ç›´æ¥ä½¿ç”¨ sqlite3.connectï¼Œä¸èµ° from_conn_string çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨
            conn = sqlite3.connect(config.CHAT_MEMORY_DB_PATH, check_same_thread=False)
            _checkpointer = SqliteSaver(conn)
            # åˆ›å»ºè¡¨ç»“æ„ï¼ˆè‹¥ä¸å­˜åœ¨ï¼‰
            _checkpointer.setup()
            logger.info(f"ä½¿ç”¨ SQLite Checkpointerï¼Œæ•°æ®åº“è·¯å¾„: {config.CHAT_MEMORY_DB_PATH}")
        except Exception as e:
            # æ•è·å¹¶æ‰“å°è¯¦ç»†å¼‚å¸¸ï¼Œå†é™çº§åˆ° MemorySaver
            logger.error(
                "åˆå§‹åŒ– SqliteSaver å¤±è´¥ï¼Œå°†é™çº§ä¸º MemorySaverã€‚é”™è¯¯: %s: %s",
                type(e).__name__, str(e), exc_info=True
            )
            from langgraph.checkpoint.memory import MemorySaver
            _checkpointer = MemorySaver()
            logger.warning(
                "å·²åˆ‡æ¢ä¸º MemorySaverï¼ˆå†…å­˜å­˜å‚¨ï¼Œé‡å¯åä¸¢å¤±ï¼‰ã€‚"
            )
    except Exception as e:
        # å…œåº•ï¼šä½¿ç”¨ MemorySaver
        from langgraph.checkpoint.memory import MemorySaver
        _checkpointer = MemorySaver()
        logger.warning(f"åˆå§‹åŒ– Checkpointer å¤±è´¥: {e}ï¼Œä½¿ç”¨ MemorySaverï¼ˆå†…å­˜å­˜å‚¨ï¼‰")
    
    return _checkpointer


def _get_agent():
    """æ„å»ºæˆ–è¿”å›ç¼“å­˜çš„ Agentï¼ˆé›†æˆçŸ­æœŸè®°å¿†å’Œ Summarizationï¼‰ã€‚"""
    global _agent
    if _agent is not None:
        return _agent

    llm = LLMHandler.get_instance().get_model()
    checkpointer = _get_checkpointer()
    
    # åˆ›å»º Summarization Middlewareï¼ˆè‡ªåŠ¨å‹ç¼©å†å²ï¼‰
    # åœ¨ _get_agent() ä¸­æ·»åŠ 
    summarization_middleware = SummarizationMiddleware(
        model=llm,
        max_tokens_before_summary=config.MEMORY_MAX_TOKENS_BEFORE_SUMMARY,  # ä¸´æ—¶é™ä½é˜ˆå€¼ï¼Œæ–¹ä¾¿æµ‹è¯•
        messages_to_keep=config.MEMORY_MESSAGES_TO_KEEP,  
    )

    logger.warning(f"ğŸ”¥ Summarization é…ç½®: max_tokens={config.MEMORY_MAX_TOKENS_BEFORE_SUMMARY}, keep={config.MEMORY_MESSAGES_TO_KEEP}")
    
    system_prompt = (
        "ä½ æœ‰ä¸€ä¸ªç”¨äºæ£€ç´¢ä¸Šä¸‹æ–‡çš„å·¥å…· retrieve_contextã€‚"
        "è¯·åœ¨éœ€è¦å¤–éƒ¨çŸ¥è¯†æ—¶è°ƒç”¨è¯¥å·¥å…·ï¼Œå¹¶åŸºäºè¿”å›çš„å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
        "æ— æ³•ä»ä¸Šä¸‹æ–‡ç¡®å®šç­”æ¡ˆæ—¶è¯·æ˜ç¡®è¯´æ˜ã€‚\n\n"
        "å›ç­”æ ¼å¼è¦æ±‚ï¼š\n"
        "1. ä½¿ç”¨è§„èŒƒçš„ Markdown æ ¼å¼ï¼Œç¡®ä¿æ®µè½ä¹‹é—´æœ‰ç©ºè¡Œåˆ†éš”ã€‚\n"
        "2. åˆ—è¡¨é¡¹å‰åè¦æœ‰ç©ºè¡Œï¼Œä½¿ç”¨ `-` æˆ– `1.` å¼€å¤´ã€‚\n"
        "3. ä»£ç å—ä½¿ç”¨ä¸‰ä¸ªåå¼•å·åŒ…è£¹ï¼Œå¹¶æ ‡æ³¨è¯­è¨€ã€‚\n"
        "4. é•¿æ®µè½è¦é€‚å½“åˆ†æ®µï¼Œæé«˜å¯è¯»æ€§ã€‚"
    )
    
    _agent = create_agent(
        llm,
        [retrieve_context],
        system_prompt=system_prompt,
        checkpointer=checkpointer,
        middleware=[summarization_middleware],
    )
    logger.info("RAG agent å·²åˆ›å»ºå¹¶ç¼“å­˜ï¼ˆå«çŸ­æœŸè®°å¿†å’Œ Summarizationï¼‰ã€‚")
    return _agent


# ---------------------------- å¯¹å¤–æ¥å£ ----------------------------
def invoke(question: str, thread_id: str = "1", timeout_s: Optional[float] = None) -> str:
    """
    ä¸€æ¬¡æ€§è°ƒç”¨ï¼Œè¿”å›å®Œæ•´å›ç­”æ–‡æœ¬ã€‚
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        thread_id: å¯¹è¯çº¿ç¨‹IDï¼Œç”¨äºåŒºåˆ†ä¸åŒä¼šè¯ï¼ˆé»˜è®¤ "1"ï¼‰
        timeout_s: è¶…æ—¶æ—¶é—´ï¼ˆæš‚æœªä½¿ç”¨ï¼‰
    
    Returns:
        å®Œæ•´å›ç­”æ–‡æœ¬
    """
    agent = _get_agent()
    config_dict = {"configurable": {"thread_id": thread_id}}
    
    # é‡‡ç”¨ messages æµï¼Œåªæ‹¼æ¥æ¨¡å‹æ–‡æœ¬å—
    parts: List[str] = []
    for token, meta in agent.stream(
        {"messages": [{"role": "user", "content": question}]},
        stream_mode="messages",
        config=config_dict,
    ):
        if isinstance(meta, dict) and meta.get("langgraph_node") != "model":
            continue
        for block in getattr(token, "content_blocks", []) or []:
            if isinstance(block, dict) and block.get("type") == "text":
                text = block.get("text", "")
                if text:
                    parts.append(text)
    return "".join(parts)


def stream_updates(question: str, thread_id: str = "1"):
    """
    æ­¥éª¤çº§æµï¼ˆmodel â†’ tools â†’ modelï¼‰ã€‚äº§å‡º dictï¼Œä¾¿äºè°ƒè¯•ä¸è§‚æµ‹ã€‚
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        thread_id: å¯¹è¯çº¿ç¨‹IDï¼Œç”¨äºåŒºåˆ†ä¸åŒä¼šè¯ï¼ˆé»˜è®¤ "1"ï¼‰
    
    Yields:
        æ­¥éª¤æ›´æ–°å­—å…¸
    """
    agent = _get_agent()
    config_dict = {"configurable": {"thread_id": thread_id}}
    for chunk in agent.stream(
        {"messages": [{"role": "user", "content": question}]},
        stream_mode="updates",
        config=config_dict,
    ):
        yield chunk


def stream_messages(question: str, thread_id: str = "1"):
    """
    ä»…æµå¼è¾“å‡ºæ¨¡å‹æ–‡æœ¬å—ï¼ˆé€æ®µï¼‰ã€‚äº§å‡º strã€‚
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        thread_id: å¯¹è¯çº¿ç¨‹IDï¼Œç”¨äºåŒºåˆ†ä¸åŒä¼šè¯ï¼ˆé»˜è®¤ "1"ï¼‰
    
    Yields:
        æ–‡æœ¬å—å­—ç¬¦ä¸²
    """
    agent = _get_agent()
    config_dict = {"configurable": {"thread_id": thread_id}}
    for token, meta in agent.stream(
        {"messages": [{"role": "user", "content": question}]},
        stream_mode="messages",
        config=config_dict,
    ):
        if isinstance(meta, dict) and meta.get("langgraph_node") != "model":
            continue
        blocks = getattr(token, "content_blocks", None)
        if not blocks:
            continue
        for block in blocks:
            if isinstance(block, dict) and block.get("type") == "text":
                text = block.get("text", "")
                if text:
                    yield text


