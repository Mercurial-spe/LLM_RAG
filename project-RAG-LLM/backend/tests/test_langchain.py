"""
LangChain è¯»å–æµ‹è¯•
=================
æœ¬æµ‹è¯•ä»…é€šè¿‡ LangChain çš„ Retriever æ¥å£ä»æœ¬åœ° Chroma é›†åˆè¯»å–ï¼Œ
ä¸è¿›è¡Œä»»ä½•å†™å…¥æˆ–é‡æ–°ç´¢å¼•ã€‚æ ¸å¿ƒåŒºåˆ«ï¼šä¸è°ƒç”¨æˆ‘ä»¬è‡ªå®šä¹‰çš„ç›¸ä¼¼åº¦æ£€ç´¢ï¼Œ
è€Œæ˜¯ä½¿ç”¨ `VectorStoreRepository.as_langchain_retriever(...)` è¿”å›çš„
LangChain Retriever å®Œæˆæ£€ç´¢ã€‚
"""

import logging
import sys
from pathlib import Path
from typing import List

# --- å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„ä¸­ ---
PROJECT_ROOT = Path(__file__).resolve().parents[2]
sys.path.append(str(PROJECT_ROOT))

# --- å¯¼å…¥é¡¹ç›®å†…æ¨¡å— ---
try:
    from backend.app.services.embedding_service import EmbeddingService
    from backend.app.services.vector_store_repository import VectorStoreRepository
except ImportError as e:
    print(f"å¯¼å…¥æ¨¡å—å¤±è´¥ï¼Œè¯·æ£€æŸ¥ PYTHONPATH æ˜¯å¦æ­£ç¡®è®¾ç½®: {e}")
    print(f"PROJECT_ROOT (å·²æ·»åŠ åˆ° sys.path): {PROJECT_ROOT}")
    sys.exit(1)

# --- LangChain æ¥å£ ---
from langchain_core.embeddings import Embeddings
from langchain.tools import tool
from langchain.agents import create_agent


# --- æ—¥å¿—é…ç½®ï¼ˆæŠ‘åˆ¶ç¬¬ä¸‰æ–¹å™ªå£°ï¼Œèšç„¦ç»“æœå±•ç¤ºï¼‰ ---
logging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
logging.getLogger("chromadb").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("openai").setLevel(logging.WARNING)


class LCEmbeddingAdapter(Embeddings):
    """å°†é¡¹ç›®å†…çš„ EmbeddingService é€‚é…ä¸º LangChain Embeddings æ¥å£ã€‚

    ä»…å®ç°æŸ¥è¯¢ä¸æ–‡æ¡£åµŒå…¥æ‰€éœ€çš„æœ€å°æ–¹æ³•ï¼š
      - embed_documents
      - embed_query
    """

    def __init__(self, service: EmbeddingService):
        self._service = service

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return self._service.embed_texts(texts)

    def embed_query(self, text: str) -> List[float]:
        return self._service.embed_text(text)


TEST_QUESTIONS = [
    "è¯´æ˜è®¡ç®—æœºç½‘ç»œçš„å­¦ä¹ å†…å®¹ã€‚",
    "TCP å’Œ UDP æœ‰ä»€ä¹ˆä¸»è¦åŒºåˆ«ï¼Ÿ",
    "è¯·è¯¦ç»†è§£é‡Šä¸€ä¸‹ TCP çš„ä¸‰æ¬¡æ¡æ‰‹è¿‡ç¨‹ã€‚",
]


if __name__ == "__main__":
    logger.info("=" * 60)
    logger.info("ğŸš€ å¼€å§‹ LangChain Retriever æœ¬åœ°è¯»å–æµ‹è¯•ï¼ˆä»…è¯»ï¼Œä¸å†™ï¼‰")
    logger.info("=" * 60)

    try:
        # 1) åˆå§‹åŒ–æœåŠ¡
        logger.info("[1] åˆå§‹åŒ– EmbeddingService ä¸ VectorStoreRepository ...")
        embedding_service = EmbeddingService.get_instance()
        vector_repo = VectorStoreRepository()

        # 2) æ„å»º LangChain Embeddings é€‚é…å™¨ä¸ Retriever
        logger.info("[2] æ„å»º LangChain Embeddings é€‚é…å™¨ä¸ Retriever ...")
        lc_embeddings = LCEmbeddingAdapter(embedding_service)

        retriever = vector_repo.as_langchain_retriever(
            embedding_instance=lc_embeddings,
            search_type="similarity",
            search_kwargs={"k": 5},
        )
        logger.info("âœ“ Retriever æ„å»ºæˆåŠŸï¼ˆé™„ç€åˆ°ç°æœ‰ Chroma é›†åˆï¼Œä»…æ‰§è¡Œè¯»å–ï¼‰")

        # 3) åŸºäº Agent çš„å®ç°ï¼šå°†æ£€ç´¢æ³¨å†Œä¸ºå·¥å…·ï¼Œç”± Agent è‡ªä¸»è°ƒç”¨
        logger.info("[3] æ„å»º Agentï¼ˆæ£€ç´¢ä½œä¸ºå·¥å…·ï¼‰ ...")
        from backend.app.core.llm_handler import LLMHandler
        llm = LLMHandler.get_instance().get_model()

        @tool("retrieve_context", response_format="content_and_artifact")
        def retrieve_context(query: str):
            """æ£€ç´¢ä¸é—®é¢˜ç›¸å…³çš„ä¸Šä¸‹æ–‡å†…å®¹ã€‚è¿”å›æ‹¼æ¥æ–‡æœ¬ä»¥åŠåŸå§‹æ–‡æ¡£åˆ—è¡¨ã€‚"""
            retrieved_docs = retriever.invoke(query)
            serialized = "\n\n".join(
                (f"Source: {doc.metadata.get('source', '<unknown>')}\nContent: {doc.page_content}")
                for doc in retrieved_docs
            )
            return serialized, retrieved_docs

        tools = [retrieve_context]
        system_prompt = (
            "ä½ æœ‰ä¸€ä¸ªç”¨äºæ£€ç´¢ä¸Šä¸‹æ–‡çš„å·¥å…· retrieve_contextã€‚"
            "è¯·åœ¨éœ€è¦å¤–éƒ¨çŸ¥è¯†æ—¶è°ƒç”¨è¯¥å·¥å…·ï¼Œå¹¶åŸºäºè¿”å›çš„å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
            "æ— æ³•ä»ä¸Šä¸‹æ–‡ç¡®å®šç­”æ¡ˆæ—¶è¯·æ˜ç¡®è¯´æ˜ã€‚"
        )

        agent = create_agent(llm, tools, system_prompt=system_prompt)

        # 4) æ‰§è¡ŒæŸ¥è¯¢ï¼ˆä»…æµå¼ LLM æ–‡æœ¬è¾“å‡ºï¼‰ï¼šä¸æ˜¾ç¤ºå·¥å…·æ­¥éª¤
        logger.info(f"[4] å‡†å¤‡æ‰§è¡Œ {len(TEST_QUESTIONS)} ä¸ªæŸ¥è¯¢ ...")
        for i, question in enumerate(TEST_QUESTIONS, start=1):
            logger.info("\n" + "=" * 24 + f" [ æµ‹è¯• {i}/{len(TEST_QUESTIONS)} ] " + "=" * 24)
            logger.info(f"  [é—®é¢˜]: {question}")

            print("\n" + "-" * 20 + " Agent Streaming (LLM messages) " + "-" * 20)
            final_text = []
            for token, metadata in agent.stream(
                {"messages": [{"role": "user", "content": question}]},
                stream_mode="messages",
            ):
                # åªå…³æ³¨æ¨¡å‹èŠ‚ç‚¹äº§ç”Ÿçš„æ–‡æœ¬å—ï¼ˆå¿½ç•¥å·¥å…·ä¸éæ–‡æœ¬å—ï¼‰
                node = metadata.get("langgraph_node") if isinstance(metadata, dict) else None
                content_blocks = getattr(token, "content_blocks", None)
                if node == "model" and content_blocks:
                    for block in content_blocks:
                        if isinstance(block, dict) and block.get("type") == "text":
                            text = block.get("text", "")
                            if text:
                                final_text.append(text)
                                print(text, end="", flush=True)

            print("\n" + "-" * 20 + " Agent æœ€ç»ˆå›ç­”ï¼ˆåˆå¹¶ï¼‰ " + "-" * 20)
            print("".join(final_text) or "<æ— è¾“å‡º>")

        logger.info("\n" + "=" * 60)
        logger.info("ğŸ‰ LangChain è¯»å–æµ‹è¯•æ‰§è¡Œå®Œæ¯•")
        logger.info("=" * 60)

    except Exception as e:
        logger.error(f"\nâœ— LangChain è¯»å–æµ‹è¯•å¤±è´¥: {e}", exc_info=True)

